{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1716748723058,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"},"user_tz":-330},"id":"6wuouANnLoiJ","outputId":"c7e44d2b-8d99-4c3c-cab9-d3f270aaeba2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal Policy (Value Iteration): [0 0 0]\n","Value Function (Value Iteration): [7.57356278 7.77248457 8.25188349]\n","Optimal Policy (Policy Iteration): [0 0 0]\n","Value Function (Policy Iteration): [7.57356346 7.77248525 8.25188414]\n","Optimal Policy (Q-Learning): [1 0 0]\n","Q-Table (Q-Learning): [[5.21292141 6.82114534]\n"," [7.39741234 6.76682418]\n"," [9.24107359 6.68600706]]\n"]}],"source":["def value_iteration(P, R, gamma=0.9, epsilon=1e-6):\n","    n_states, n_actions = R.shape[1], R.shape[0]\n","    V = np.zeros(n_states)\n","    while True:\n","        delta = 0\n","        for s in range(n_states):\n","            v = V[s]\n","            V[s] = max(sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions))\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < epsilon:\n","            break\n","    policy = np.argmax([[sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions)] for s in range(n_states)], axis=1)\n","    return policy, V"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal Policy (Value Iteration): [0 1 1]\n","Value Function (Value Iteration): [6.22147264 5.98458936 6.18126588]\n"]}],"source":["import numpy as np\n","import random\n","\n","### MDP Algorithms ###\n","def value_iteration(P, R, gamma=0.9, epsilon=1e-6):\n","    n_states, n_actions = R.shape[1], R.shape[0]\n","    V = np.zeros(n_states)\n","    while True:\n","        delta = 0\n","        for s in range(n_states):\n","            v = V[s]\n","            V[s] = max(sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions))\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < epsilon:\n","            break\n","    policy = np.argmax([[sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions)] for s in range(n_states)], axis=1)\n","    return policy, V\n","\n","def policy_iteration(P, R, gamma=0.9, epsilon=1e-6):\n","    num_states, num_actions = R.shape[1], R.shape[0]\n","    policy = np.zeros(num_states, dtype=int)\n","    V = np.zeros(num_states)\n","    while True:\n","        while True:\n","            delta = 0\n","            for s in range(num_states):\n","                v = V[s]\n","                V[s] = sum(P[policy[s], s, s1] * (R[policy[s], s] + gamma * V[s1]) for s1 in range(num_states))\n","                delta = max(delta, abs(v - V[s]))\n","            if delta < epsilon:\n","                break\n","            policy_stable = True\n","            for s in range(num_states):\n","                old_action = policy[s]\n","                policy[s] = np.argmax([sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(num_states)) for a in range(num_actions)])\n","                if old_action != policy[s]:\n","                    policy_stable = False\n","            if policy_stable:\n","                break\n","    return policy, V\n","\n","def q_learning(P, R, gamma=0.9, alpha=0.1, epsilon=0.1, episodes=1000):\n","    num_states, num_actions = R.shape[1], R.shape[0]\n","    Q = np.zeros((num_states, num_actions))\n","    for _ in range(episodes):\n","        state = random.choice(range(num_states))\n","        while True:\n","            if random.uniform(0, 1) < epsilon:\n","                action = random.choice(range(num_actions))\n","            else:\n","                action = np.argmax(Q[state])\n","            next_state = np.argmax(P[action, state])\n","            reward = R[action, state]\n","            best_next_action = np.argmax(Q[next_state])\n","            td_target = reward + gamma * Q[next_state, best_next_action]\n","            td_error = td_target - Q[state, action]\n","            Q[state, action] += alpha * td_error\n","            if state == next_state:\n","                break\n","            state = next_state\n","    policy = np.argmax(Q, axis=1)\n","    return policy, Q\n","\n","### Utility Functions ###\n","\n","def validate_transition_matrix(P):\n","    assert np.allclose(P.sum(axis=2), 1), \"Transition probabilities must sum to 1.\"\n","\n","def validate_reward_matrix(R, P):\n","    assert R.shape == P.shape[:2], \"Reward matrix dimensions must match the transition matrix.\"\n","\n","def generate_random_mdp(num_states, num_actions):\n","    P = np.zeros((num_actions, num_states, num_states))\n","    for a in range(num_actions):\n","        for s in range(num_states):\n","            P[a, s, :] = np.random.dirichlet(np.ones(num_states))\n","    R = np.random.rand(num_actions, num_states)\n","    return P, R\n","\n","### Example Usage ###\n","\n","# Generate a random MDP\n","num_states = 3\n","num_actions = 2\n","P, R = generate_random_mdp(num_states, num_actions)\n","\n","# Validate the MDP\n","validate_transition_matrix(P)\n","validate_reward_matrix(R, P)\n","\n","# Solve the MDP using Value Iteration\n","policy_vi, V_vi = value_iteration(P, R)\n","print(\"Optimal Policy (Value Iteration):\", policy_vi)\n","print(\"Value Function (Value Iteration):\", V_vi)\n","\n","# Solve the MDP using Policy Iteration\n","policy_pi, V_pi = policy_iteration(P, R)\n","print(\"Optimal Policy (Policy Iteration):\", policy_pi)\n","print(\"Value Function (Policy Iteration):\", V_pi)\n","\n","# Solve the MDP using Q-Learning\n","policy_ql, Q_ql = q_learning(P, R)\n","print(\"Optimal Policy (Q-Learning):\", policy_ql)\n","print(\"Q-Table (Q-Learning):\", Q_ql)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCUiVh+tkRk6ZhOoinaxp8","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
