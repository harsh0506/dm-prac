{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","\u001b[1;31mUnable to start Kernel 'Python 3.12.3' due to a timeout waiting for the ports to get used. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["! pip install gym"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94537,"status":"ok","timestamp":1716751118839,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"},"user_tz":-330},"id":"XkfVgtK_TX_9","outputId":"20b268b6-ec75-4546-b404-c207edc2de07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 1 Reward: -1495.760387698824\n","Episode: 2 Reward: -1317.6043848744087\n","Episode: 3 Reward: -1148.501590913999\n","Episode: 4 Reward: -1255.3839254633124\n","Episode: 5 Reward: -944.9861228986489\n","Episode: 6 Reward: -1559.6234125688973\n","Episode: 7 Reward: -1171.1532151987162\n","Episode: 8 Reward: -1729.408661360246\n","Episode: 9 Reward: -1639.4917373002643\n","Episode: 10 Reward: -1371.8198135753485\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import gym\n","\n","# Actor Model\n","class Actor(tf.keras.Model):\n","    def __init__(self, state_dim, action_dim, action_bound):\n","        super(Actor, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(action_dim, activation='tanh')\n","        self.action_bound = action_bound\n","\n","    def call(self, inputs):\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","# Critic Model\n","class Critic(tf.keras.Model):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(1)\n","\n","    def call(self, inputs):\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","# Actor-Critic Agent\n","class ActorCriticAgent:\n","    def __init__(self, state_dim, action_dim, action_bound, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n","        self.actor = Actor(state_dim, action_dim, action_bound)\n","        self.critic = Critic()\n","        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n","        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n","        self.gamma = gamma\n","\n","    def get_action(self, state):\n","        return self.actor(tf.convert_to_tensor([state])).numpy()[0]\n","\n","    def train(self, states, actions, rewards, next_states, dones):\n","        # Compute TD targets\n","        next_q_values = self.critic(tf.convert_to_tensor(next_states, dtype=tf.float32))\n","        targets = rewards + (1 - dones) * self.gamma * next_q_values.numpy().flatten()\n","\n","        # Compute advantages\n","        values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32)).numpy().flatten()\n","        advantages = targets - values\n","\n","        # Train actor\n","        with tf.GradientTape() as tape:\n","            actor_actions = self.actor(tf.convert_to_tensor(states, dtype=tf.float32))\n","            actor_loss = -tf.reduce_mean(self.critic(tf.convert_to_tensor(states, dtype=tf.float32)) * actor_actions)\n","        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n","\n","        # Train critic\n","        with tf.GradientTape() as tape:\n","            critic_values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32))\n","            critic_loss = tf.reduce_mean(tf.square(targets - critic_values))\n","        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n","        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n","\n","    def train_episode(self, episode):\n","        state = self.env.reset()\n","        episode_reward = 0\n","        while True:\n","            action = self.get_action(state)\n","            next_state, reward, done, _ = self.env.step(action)\n","            self.train(state, action, reward, next_state, done)\n","            episode_reward += reward\n","            state = next_state\n","            if done:\n","                print(f\"Episode {episode + 1} Reward: {episode_reward}\")\n","                break\n","\n","# Example Usage\n","env = gym.make('Pendulum-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","action_bound = env.action_space.high[0]\n","\n","agent = ActorCriticAgent(state_dim, action_dim, action_bound)\n","\n","episodes = 10\n","for episode in range(episodes):\n","    agent.train_episode(episode)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOuyLPVaqbuEUl09EUkCWDa","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
